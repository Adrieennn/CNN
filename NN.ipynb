{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project: Evaluation of neural network architectures on MNIST datasets\n",
    "\n",
    "Group Members : Yidong HUANG\n",
    "                Simon ROBER\n",
    "                Marck-Edward KEMEH\n",
    "\n",
    "ABOUT\n",
    "The MNIST dataset is a large database of handwritten digits used in many forms of image processing.\n",
    "This dataset contains 60,000 training images and 10,000 test images. Our aim is to design some neural \n",
    "networks that can recognise these hand-written digits. our peojects will be based on two different networks\n",
    "for this task.                   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We begin by splitting the MNIST dataset into two. Aset for training and another for testing.\n",
    "we then normalize the test set and training set by dividing with 255 so that the values can be between 0 and 1.\n",
    "Becuase we will be working with convolutional neural network, we need to chnge the original shape of the MNIST \n",
    "dataset which is (60000, 28, 28) to that of the convolutional neural network (60000, 28, 28, 1).\n",
    "tis section is just to prepare our dataset for the network.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before adding dimension is : (60000, 28, 28)\n",
      "shape after adding dimension is : (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "print('shape before adding dimension is :' ,x_train.shape)\n",
    "x_train, x_test = np.expand_dims(x_train, axis= -1), np.expand_dims(x_test, axis = -1)\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1)\n",
    "print ('shape after adding dimension is :' ,x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''our first model is a simple network to train and test or images.\n",
    "our input shape has to match that of the train set as did above and we are using relu as activation\n",
    "The convolutional layers play an important role in the learning ability of the network. Having more \n",
    "layers might improve the learning of the datasets hence have a high accuracy on the test sets.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                11530     \n",
      "=================================================================\n",
      "Total params: 104,202\n",
      "Trainable params: 104,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#first simple model\n",
    "inputs = layers.Input(shape = (28,28,1))\n",
    "conv1 = layers.Conv2D(32, (3,3), padding ='valid', activation = 'relu')(inputs)\n",
    "pool1 = layers.MaxPool2D((2,2), (2, 2))(conv1)\n",
    "conv2 = layers.Conv2D(64, (3,3), padding ='valid', activation = 'relu')(pool1)\n",
    "pool2 = layers.MaxPool2D((2,2), (2, 2))(conv2)\n",
    "conv3 = layers.Conv2D(128, (3,3), padding ='valid', activation = 'relu')(pool2)\n",
    "\n",
    "flat = layers.Flatten()(conv3)\n",
    "outputs = layers.Dense(10, activation = 'softmax')(flat)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\",\n",
    "                metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "having a single convolutional layers with 32 filters gives us a test accuracy of 0.975.\n",
    "adding another convolutional layer with the 64 number of filters increased the test accuracy to 0.984.\n",
    "adding a third layer with 124 filters and training 4 epochs increased the accuracy to 0.990\n",
    "This tells us that more epochs also increases the learning accuracy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 56s 941us/sample - loss: 0.1385 - accuracy: 0.9587\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 58s 967us/sample - loss: 0.0432 - accuracy: 0.9867\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.0303 - accuracy: 0.9904\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 56s 926us/sample - loss: 0.0226 - accuracy: 0.9924\n",
      "10000/10000 [==============================] - 3s 346us/sample - loss: 0.0301 - accuracy: 0.9900\n",
      "test loss is : 0.030063047241355525 - test accuracy is : 0.9900000095367432\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "model.fit(x_train, y_train, epochs = 4)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"test loss is : {0} - test accuracy is : {1}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''our second model is the LeNet network which has an input shape of (32, 32, 1).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 15, 15, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 81,194\n",
      "Trainable params: 81,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#second model\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=10, activation = 'softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\",\n",
    "                metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.2296 - accuracy: 0.9301\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 143s 2ms/sample - loss: 0.0721 - accuracy: 0.9774\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 140s 2ms/sample - loss: 0.0521 - accuracy: 0.9838\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 144s 2ms/sample - loss: 0.0399 - accuracy: 0.9876\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d875ffc291a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test loss is : {0} - test accuracy is : {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "model.fit(x_train, y_train, epochs = 4)\n",
    "print(\"test loss is : {0} - test accuracy is : {1}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
